{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"W6YAEurcb8kH"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import tensorflow_hub as hub\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import os\n","import shutil\n","from sklearn.model_selection import train_test_split\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","valid_datagen = ImageDataGenerator(rescale=1./255)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5432,"status":"ok","timestamp":1700846836620,"user":{"displayName":"cristian cuevas (Tuprofeestadistica)","userId":"05899195668072443819"},"user_tz":240},"id":"-ui_fOZOytM9","outputId":"21bd8970-dd69-44c6-ab12-29cec25ef8a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"do-9dYOj6xHV"},"outputs":[],"source":["# Definir las carpetas de origen y destino\n","source_dir = \"/content/drive/MyDrive/Colab Notebooks/clasificacion imagenes Redes neuronales/dataset\"\n","train_dir = \"/content/drive/MyDrive/Colab Notebooks/clasificacion imagenes Redes neuronales/dataset/train\"\n","test_dir = \"/content/drive/MyDrive/Colab Notebooks/clasificacion imagenes Redes neuronales/dataset/test\"\n","\n","# Crear las carpetas de entrenamiento y prueba si no existen\n","os.makedirs(train_dir, exist_ok=True)\n","os.makedirs(test_dir, exist_ok=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AFmiKabM1An"},"outputs":[],"source":["# Para cada subcarpeta en la carpeta de origen\n","for class_name in ['chavez', 'maduro', 'nofigura']:\n","    # Crear las carpetas correspondientes en las carpetas de entrenamiento y prueba\n","    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n","    os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n","\n","    # Obtener la lista de imágenes en la subcarpeta\n","    images = os.listdir(os.path.join(source_dir, class_name))\n","\n","    # Dividir las imágenes en conjuntos de entrenamiento y prueba\n","    train_images, test_images = train_test_split(images, test_size=0.2, random_state=42)\n","\n","    # Mover las imágenes a las carpetas de entrenamiento y prueba correspondientes\n","    for image in train_images:\n","        shutil.move(os.path.join(source_dir, class_name, image), os.path.join(train_dir, class_name, image))\n","    for image in test_images:\n","        shutil.move(os.path.join(source_dir, class_name, image), os.path.join(test_dir, class_name, image))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":522,"status":"ok","timestamp":1700846854365,"user":{"displayName":"cristian cuevas (Tuprofeestadistica)","userId":"05899195668072443819"},"user_tz":240},"id":"U2zm5f3TJnLG","outputId":"351dcf72-f7f3-472b-b18d-91c75d16ce97"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 600 images belonging to 3 classes.\n"]}],"source":["train_data = train_datagen.flow_from_directory(directory=train_dir,\n","                                               target_size=(224, 224),\n","                                               class_mode=\"categorical\",\n","                                               seed=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1019,"status":"ok","timestamp":1700846860193,"user":{"displayName":"cristian cuevas (Tuprofeestadistica)","userId":"05899195668072443819"},"user_tz":240},"id":"ZsPf9OvlLaBz","outputId":"f4aab072-df4f-471d-eeb1-0bccad703b47"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 150 images belonging to 3 classes.\n"]}],"source":["valid_data = valid_datagen.flow_from_directory(directory=test_dir,\n","                                               batch_size=16,\n","                                               target_size=(224, 224),\n","                                               class_mode=\"categorical\",\n","                                               seed=42)"]},{"cell_type":"markdown","source":["# Modelo 1"],"metadata":{"id":"QdJngN_pMm3Q"}},{"cell_type":"markdown","source":["## Se implementó una red neuronal convolucional (CNN) sencilla que recuerda a una versión reducida de VGG, tambien referida como Tiny VGG."],"metadata":{"id":"Vy3zH1IjMo12"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUg4-hFBQ4QG"},"outputs":[],"source":["model_1 = tf.keras.models.Sequential([tf.keras.layers.Conv2D(filters=10,kernel_size=3, activation=\"relu\",input_shape=(224, 224, 3)),\n","  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n","  tf.keras.layers.MaxPool2D(pool_size=2,\n","                            padding=\"valid\"),\n","  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(10),\n","  tf.keras.layers.Dense(3, activation=\"softmax\")\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rx6dodYrQ6Ar"},"outputs":[],"source":["# Compile our CNN\n","model_1.compile(loss=\"categorical_crossentropy\",\n","                optimizer=tf.keras.optimizers.Adam(),\n","                metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":471611,"status":"ok","timestamp":1700843308643,"user":{"displayName":"cristian cuevas (Tuprofeestadistica)","userId":"05899195668072443819"},"user_tz":240},"id":"oFt_dityQ9Zs","outputId":"732def04-8660-46dc-f461-962159d72f34"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/6\n","19/19 [==============================] - 226s 12s/step - loss: 2.4740 - accuracy: 0.6350 - val_loss: 0.2860 - val_accuracy: 0.9533\n","Epoch 2/6\n","19/19 [==============================] - 37s 2s/step - loss: 0.1047 - accuracy: 0.9783 - val_loss: 0.0675 - val_accuracy: 0.9800\n","Epoch 3/6\n","19/19 [==============================] - 36s 2s/step - loss: 0.0206 - accuracy: 0.9983 - val_loss: 0.0369 - val_accuracy: 0.9800\n","Epoch 4/6\n","19/19 [==============================] - 37s 2s/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.0417 - val_accuracy: 0.9800\n","Epoch 5/6\n","19/19 [==============================] - 40s 2s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 1.0000\n","Epoch 6/6\n","19/19 [==============================] - 40s 2s/step - loss: 5.5508e-04 - accuracy: 1.0000 - val_loss: 0.0108 - val_accuracy: 0.9933\n"]}],"source":["# Fit the model\n","history_1 = model_1.fit(train_data,\n","                        epochs=6,\n","                        steps_per_epoch=len(train_data),\n","                        validation_data=valid_data,\n","                        validation_steps=len(valid_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"siTPNzKpS2Wg"},"outputs":[],"source":["model_1.save(\"/content/drive/MyDrive/Colab Notebooks/clasificacion imagenes Redes neuronales/save_model/modelo_1_final.keras\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yZaaLPOR0j3"},"outputs":[],"source":["model_1.save_weights(\"/content/drive/MyDrive/Colab Notebooks/clasificacion imagenes Redes neuronales/save_model/modelo_1_peso_final.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":316,"status":"ok","timestamp":1700846307147,"user":{"displayName":"cristian cuevas (Tuprofeestadistica)","userId":"05899195668072443819"},"user_tz":240},"id":"q1OALXsZVrGS","outputId":"c886876e-339f-482e-8514-ef52ebbb1aa1"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["model_1.save(\"/content/drive/MyDrive/Colab Notebooks/clasificacion imagenes Redes neuronales/save_model/modelo_1_final2.h5\")"]},{"cell_type":"markdown","source":["# Modelo 2"],"metadata":{"id":"2IH_JVsKMixO"}},{"cell_type":"markdown","metadata":{"id":"FCLmzEBXbNjZ"},"source":["# Se implementó la arquitectura EfficientNet B0 como base y agregando capas personalizadas para el problema específico de clasificación\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1700846905129,"user":{"displayName":"cristian cuevas (Tuprofeestadistica)","userId":"05899195668072443819"},"user_tz":240},"id":"AZdgGru0SiQt","outputId":"47bbadb3-a473-4b6d-fcb0-68dcb304d775"},"outputs":[{"name":"stdout","output_type":"stream","text":["['chavez' 'maduro' 'nofigura']\n"]}],"source":["import pathlib\n","data_dir = pathlib.Path(train_dir)\n","class_names = np.array(sorted([item.name for item in data_dir.glob('*')]))\n","print(class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9384,"status":"ok","timestamp":1700846917752,"user":{"displayName":"cristian cuevas (Tuprofeestadistica)","userId":"05899195668072443819"},"user_tz":240},"id":"rYMG7vGRbeDQ","outputId":"f31b8403-3ca4-4819-f98e-feb31d3a0317"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.6)\n","Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.14.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n","Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.7.22)\n"]}],"source":["!pip install keras-tuner"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3ZUxxG2bjv9"},"outputs":[],"source":["from keras_tuner.tuners import RandomSearch\n","from tensorflow.keras.applications import EfficientNetB0\n","import tensorflow as tf\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ZwDAvb0WGDoG","outputId":"72c47dca-991e-4809-a3af-f1dd20bbaa52"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Search: Running Trial #1\n","\n","Value             |Best Value So Far |Hyperparameter\n","1                 |1                 |num_layers\n","480               |480               |units_0\n","0.15773           |0.15773           |dropout_0\n","0.0001            |0.0001            |learning_rate\n","\n","Epoch 1/30\n","19/19 [==============================] - 263s 12s/step - loss: 0.7253 - accuracy: 0.8833 - val_loss: 1.4053 - val_accuracy: 0.3333\n","Epoch 2/30\n","19/19 [==============================] - 224s 12s/step - loss: 0.2997 - accuracy: 0.9983 - val_loss: 1.3586 - val_accuracy: 0.3333\n","Epoch 3/30\n","19/19 [==============================] - 225s 12s/step - loss: 0.2584 - accuracy: 1.0000 - val_loss: 1.3638 - val_accuracy: 0.3333\n","Epoch 4/30\n","19/19 [==============================] - 190s 10s/step - loss: 0.2553 - accuracy: 1.0000 - val_loss: 1.3663 - val_accuracy: 0.3333\n","Epoch 5/30\n","19/19 [==============================] - 173s 9s/step - loss: 0.2524 - accuracy: 1.0000 - val_loss: 1.3620 - val_accuracy: 0.3333\n","Epoch 6/30\n","19/19 [==============================] - 174s 9s/step - loss: 0.2495 - accuracy: 1.0000 - val_loss: 1.3606 - val_accuracy: 0.3067\n","Epoch 7/30\n","19/19 [==============================] - 179s 10s/step - loss: 0.2470 - accuracy: 1.0000 - val_loss: 1.3643 - val_accuracy: 0.3333\n","Epoch 8/30\n","19/19 [==============================] - 175s 9s/step - loss: 0.2463 - accuracy: 1.0000 - val_loss: 1.3579 - val_accuracy: 0.3333\n","Epoch 9/30\n","19/19 [==============================] - 173s 9s/step - loss: 0.2483 - accuracy: 0.9983 - val_loss: 1.3401 - val_accuracy: 0.3400\n","Epoch 10/30\n","19/19 [==============================] - 174s 9s/step - loss: 0.2425 - accuracy: 1.0000 - val_loss: 1.3343 - val_accuracy: 0.4067\n","Epoch 11/30\n","19/19 [==============================] - 174s 9s/step - loss: 0.2435 - accuracy: 0.9983 - val_loss: 1.3520 - val_accuracy: 0.2533\n","Epoch 12/30\n","19/19 [==============================] - 173s 9s/step - loss: 0.2400 - accuracy: 1.0000 - val_loss: 1.3394 - val_accuracy: 0.3733\n","Epoch 13/30\n","19/19 [==============================] - 173s 9s/step - loss: 0.2393 - accuracy: 0.9983 - val_loss: 1.3436 - val_accuracy: 0.3200\n","Epoch 14/30\n","19/19 [==============================] - 173s 9s/step - loss: 0.2382 - accuracy: 1.0000 - val_loss: 1.3129 - val_accuracy: 0.3200\n","Epoch 15/30\n","19/19 [==============================] - 175s 9s/step - loss: 0.2351 - accuracy: 1.0000 - val_loss: 1.2925 - val_accuracy: 0.4333\n","Epoch 16/30\n","19/19 [==============================] - 174s 9s/step - loss: 0.2333 - accuracy: 1.0000 - val_loss: 1.2653 - val_accuracy: 0.4533\n","Epoch 17/30\n","19/19 [==============================] - 188s 10s/step - loss: 0.2320 - accuracy: 1.0000 - val_loss: 1.2561 - val_accuracy: 0.4400\n","Epoch 18/30\n","19/19 [==============================] - 182s 10s/step - loss: 0.2304 - accuracy: 1.0000 - val_loss: 1.2426 - val_accuracy: 0.4800\n","Epoch 19/30\n","19/19 [==============================] - 180s 10s/step - loss: 0.2285 - accuracy: 1.0000 - val_loss: 1.2393 - val_accuracy: 0.4800\n","Epoch 20/30\n","19/19 [==============================] - 172s 9s/step - loss: 0.2267 - accuracy: 1.0000 - val_loss: 1.1922 - val_accuracy: 0.5267\n","Epoch 21/30\n","19/19 [==============================] - 177s 9s/step - loss: 0.2255 - accuracy: 1.0000 - val_loss: 1.0690 - val_accuracy: 0.6400\n","Epoch 22/30\n","19/19 [==============================] - 175s 9s/step - loss: 0.2231 - accuracy: 1.0000 - val_loss: 1.0420 - val_accuracy: 0.6000\n","Epoch 23/30\n","19/19 [==============================] - 184s 10s/step - loss: 0.2214 - accuracy: 1.0000 - val_loss: 1.0135 - val_accuracy: 0.6133\n","Epoch 24/30\n","19/19 [==============================] - 174s 9s/step - loss: 0.2197 - accuracy: 1.0000 - val_loss: 0.8909 - val_accuracy: 0.6933\n","Epoch 25/30\n","19/19 [==============================] - 176s 9s/step - loss: 0.2179 - accuracy: 1.0000 - val_loss: 0.7865 - val_accuracy: 0.7667\n","Epoch 26/30\n","19/19 [==============================] - 174s 9s/step - loss: 0.2161 - accuracy: 1.0000 - val_loss: 0.7201 - val_accuracy: 0.7733\n","Epoch 27/30\n","19/19 [==============================] - 184s 10s/step - loss: 0.2142 - accuracy: 1.0000 - val_loss: 0.6036 - val_accuracy: 0.8600\n","Epoch 28/30\n","19/19 [==============================] - 172s 9s/step - loss: 0.2124 - accuracy: 1.0000 - val_loss: 0.5107 - val_accuracy: 0.9133\n","Epoch 29/30\n","19/19 [==============================] - 176s 9s/step - loss: 0.2105 - accuracy: 1.0000 - val_loss: 0.4178 - val_accuracy: 0.9800\n","Epoch 30/30\n","19/19 [==============================] - 173s 9s/step - loss: 0.2088 - accuracy: 1.0000 - val_loss: 1.0690 - val_accuracy: 0.5867\n","Epoch 1/30\n","19/19 [==============================] - 208s 10s/step - loss: 0.6745 - accuracy: 0.9117 - val_loss: 1.3610 - val_accuracy: 0.3333\n","Epoch 2/30\n","19/19 [==============================] - 175s 9s/step - loss: 0.2965 - accuracy: 0.9967 - val_loss: 1.4009 - val_accuracy: 0.3333\n","Epoch 3/30\n","19/19 [==============================] - 175s 9s/step - loss: 0.2576 - accuracy: 1.0000 - val_loss: 1.3939 - val_accuracy: 0.3333\n","Epoch 4/30\n","19/19 [==============================] - 182s 10s/step - loss: 0.2515 - accuracy: 1.0000 - val_loss: 1.3898 - val_accuracy: 0.3333\n","Epoch 5/30\n","19/19 [==============================] - 173s 9s/step - loss: 0.2501 - accuracy: 1.0000 - val_loss: 1.3950 - val_accuracy: 0.3333\n","Epoch 6/30\n","19/19 [==============================] - 174s 9s/step - loss: 0.2509 - accuracy: 0.9983 - val_loss: 1.4162 - val_accuracy: 0.3333\n","Epoch 7/30\n","19/19 [==============================] - 177s 9s/step - loss: 0.2471 - accuracy: 1.0000 - val_loss: 1.4569 - val_accuracy: 0.3333\n","Epoch 8/30\n","19/19 [==============================] - 172s 9s/step - loss: 0.2448 - accuracy: 1.0000 - val_loss: 1.4674 - val_accuracy: 0.3333\n","Epoch 9/30\n","19/19 [==============================] - 171s 9s/step - loss: 0.2443 - accuracy: 1.0000 - val_loss: 1.4370 - val_accuracy: 0.3333\n","Epoch 10/30\n","19/19 [==============================] - 166s 9s/step - loss: 0.2420 - accuracy: 1.0000 - val_loss: 1.3892 - val_accuracy: 0.3333\n","Epoch 11/30\n","19/19 [==============================] - 168s 9s/step - loss: 0.2407 - accuracy: 1.0000 - val_loss: 1.3599 - val_accuracy: 0.3667\n","Epoch 12/30\n","19/19 [==============================] - 164s 9s/step - loss: 0.2391 - accuracy: 1.0000 - val_loss: 1.3655 - val_accuracy: 0.3467\n","Epoch 13/30\n","19/19 [==============================] - 165s 9s/step - loss: 0.2376 - accuracy: 1.0000 - val_loss: 1.3879 - val_accuracy: 0.3000\n","Epoch 14/30\n","19/19 [==============================] - 164s 9s/step - loss: 0.2362 - accuracy: 1.0000 - val_loss: 1.4275 - val_accuracy: 0.2400\n","Epoch 15/30\n","19/19 [==============================] - 164s 9s/step - loss: 0.2344 - accuracy: 1.0000 - val_loss: 1.4417 - val_accuracy: 0.2400\n","Epoch 16/30\n","19/19 [==============================] - 164s 9s/step - loss: 0.2359 - accuracy: 1.0000 - val_loss: 1.4454 - val_accuracy: 0.3267\n","Epoch 17/30\n","19/19 [==============================] - 165s 9s/step - loss: 0.2317 - accuracy: 1.0000 - val_loss: 1.4199 - val_accuracy: 0.3667\n","Epoch 18/30\n","19/19 [==============================] - 164s 9s/step - loss: 0.2294 - accuracy: 1.0000 - val_loss: 1.3790 - val_accuracy: 0.3067\n","Epoch 19/30\n","19/19 [==============================] - 165s 9s/step - loss: 0.2274 - accuracy: 1.0000 - val_loss: 1.3291 - val_accuracy: 0.3800\n","Epoch 20/30\n","19/19 [==============================] - 164s 9s/step - loss: 0.2256 - accuracy: 1.0000 - val_loss: 1.2812 - val_accuracy: 0.4267\n","Epoch 21/30\n","19/19 [==============================] - 163s 9s/step - loss: 0.2241 - accuracy: 1.0000 - val_loss: 1.2095 - val_accuracy: 0.5000\n","Epoch 22/30\n","19/19 [==============================] - 172s 9s/step - loss: 0.2223 - accuracy: 1.0000 - val_loss: 1.1219 - val_accuracy: 0.5333\n","Epoch 23/30\n","19/19 [==============================] - 165s 9s/step - loss: 0.2215 - accuracy: 1.0000 - val_loss: 0.9836 - val_accuracy: 0.6267\n","Epoch 24/30\n","19/19 [==============================] - 165s 9s/step - loss: 0.2184 - accuracy: 1.0000 - val_loss: 0.8854 - val_accuracy: 0.7000\n","Epoch 25/30\n","19/19 [==============================] - 164s 9s/step - loss: 0.2164 - accuracy: 1.0000 - val_loss: 0.7867 - val_accuracy: 0.7933\n","Epoch 26/30\n"," 6/19 [========>.....................] - ETA: 1:47 - loss: 0.2151 - accuracy: 1.0000"]}],"source":["# Definir la función de construcción del modelo\n","def build_model(hp):\n","    model = tf.keras.models.Sequential()\n","    model.add(EfficientNetB0(include_top=False, weights='imagenet', input_shape=(224, 224, 3)))\n","    model.add(tf.keras.layers.GlobalAveragePooling2D())\n","\n","    for i in range(hp.Int('num_layers', 1, 5)):\n","        model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n","            min_value=32,\n","            max_value=512,\n","            step=32),\n","            activation='relu',\n","            kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n","        model.add(tf.keras.layers.Dropout(hp.Float('dropout_' + str(i), 0.0, 0.5)))\n","\n","    model.add(tf.keras.layers.Dense(3, activation='softmax'))\n","\n","    model.compile(optimizer=tf.keras.optimizers.Adam(\n","        hp.Choice('learning_rate',\n","            values=[1e-2, 1e-3, 1e-4])),\n","        loss='categorical_crossentropy',\n","        metrics=['accuracy'])\n","\n","    return model\n","\n","# Crear el sintonizador\n","tuner = RandomSearch(\n","    build_model,\n","    objective='val_accuracy',\n","    max_trials=4,\n","    executions_per_trial=3,\n","    directory='/content/drive/MyDrive/Colab Notebooks/clasificacion imagenes Redes neuronales/my_dir7',\n","    project_name='best')\n","\n","# Ejecutar el sintonizador\n","tuner.search(train_data, epochs=30, validation_data=valid_data)\n","best_model = tuner.get_best_models(num_models=1)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0S-zI5FcDnd"},"outputs":[],"source":["best_model.save(\"/content/drive/MyDrive/Colab Notebooks/clasificacion imagenes Redes neuronales/save_model/best_model_3_1.keras\")"]},{"cell_type":"markdown","metadata":{"id":"VuCsipeKIlnU"},"source":["## funciones de interes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5V1X6EeSxjj"},"outputs":[],"source":["# Create a function to import an image and resize it to be able to be used with our model\n","def load_and_prep_image(filename, img_shape=224):\n","  \"\"\"\n","  Reads in an image from filename, turns it into a tensor and reshapes into (224,224,3).\n","  \"\"\"\n","  # Read in the image\n","  img = tf.io.read_file(filename)\n","  # Decode it into a tensor\n","  img = tf.image.decode_jpeg(img)\n","  # Resize the image\n","  img = tf.image.resize(img, [img_shape, img_shape])\n","  # Rescale the image (get all values between 0 and 1)\n","  img = img/255.\n","  return img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3a6Z0aKS0Cn"},"outputs":[],"source":["# Reconfig pred_and_plot function to work with multi-class images\n","def pred_and_plot(model, filename, class_names=class_names):\n","  \"\"\"\n","  Imports an image located at filename, makes a prediction with model\n","  and plots the image with the predicted class as the title.\n","  \"\"\"\n","  # Import the target image and preprocess it\n","  img = load_and_prep_image(filename)\n","\n","  # Make a prediction\n","  pred = model.predict(tf.expand_dims(img, axis=0))\n","\n","  # Add in logic for multi-class & get pred_class name\n","  if len(pred[0]) > 1:\n","    pred_class = class_names[tf.argmax(pred[0])]\n","  else:\n","    pred_class = class_names[int(tf.round(pred[0]))]\n","\n","  print('Prediction Probabilities : ', pred[0])\n","\n","  # Plot the image and predicted class\n","  plt.imshow(img)\n","  plt.title(f\"Prediction: {pred_class}\")\n","  plt.axis(False);\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPIHDL0RoLuRZf4QdGmqHVL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}